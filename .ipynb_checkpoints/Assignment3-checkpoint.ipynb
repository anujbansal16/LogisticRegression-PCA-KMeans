{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Import Libraries\n",
    "\n",
    "Import necessary libraries used in these assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainTest(data,percent):\n",
    "    total=len(data)\n",
    "    trainTotal=int(total*percent*0.01)\n",
    "    testTotal=total-trainTotal\n",
    "    return (data[0:trainTotal],data[trainTotal:total])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.e**(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticLoss(yactual,hx):\n",
    "    return -np.mean(yactual*np.log(hx)+(1-yactual)*np.log(1-hx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "Intercept: \n",
      "[[1.05898895]]\n",
      "Coefficents: \n",
      "[[ 0.12647931]\n",
      " [ 0.10591461]\n",
      " [ 0.09204225]\n",
      " [-0.04570449]\n",
      " [ 0.07104959]\n",
      " [ 0.40226706]\n",
      " [ 0.05138659]]\n",
      "=========================================\n",
      "    Actual |  Predicted\n",
      "      0.64 | [0.64482887]\n",
      "      0.86 | [0.83846823]\n",
      "       0.8 | [0.80487618]\n",
      "      0.72 | [0.68342215]\n",
      "      0.73 | [0.8219365]\n",
      "       0.8 | [0.78760144]\n",
      "      0.81 | [0.75349781]\n",
      "      0.38 | [0.49447112]\n",
      "      0.58 | [0.6434298]\n",
      "       0.8 | [0.81100376]\n",
      "      0.96 | [0.92363821]\n",
      "      0.54 | [0.53969249]\n",
      "      0.62 | [0.62719064]\n",
      "      0.82 | [0.79626013]\n",
      "      0.51 | [0.44678763]\n",
      "      0.78 | [0.78545493]\n",
      "      0.63 | [0.71454211]\n",
      "      0.73 | [0.63051611]\n",
      "      0.52 | [0.77508782]\n",
      "      0.73 | [0.75866103]\n",
      "       0.8 | [0.80714003]\n",
      "      0.64 | [0.6097314]\n",
      "      0.94 | [0.90082909]\n",
      "      0.65 | [0.63477091]\n",
      "      0.92 | [0.89122226]\n",
      "      0.85 | [0.85784046]\n",
      "      0.95 | [0.90907019]\n",
      "      0.86 | [0.85165061]\n",
      "      0.64 | [0.6434477]\n",
      "      0.94 | [0.92222857]\n",
      "      0.61 | [0.65133832]\n",
      "      0.72 | [0.63995235]\n",
      "      0.92 | [0.89986885]\n",
      "      0.68 | [0.6695336]\n",
      "      0.73 | [0.62972264]\n",
      "      0.84 | [0.82947878]\n",
      "      0.71 | [0.73178248]\n",
      "      0.72 | [0.756989]\n",
      "      0.65 | [0.62418968]\n",
      "      0.89 | [0.86245772]\n",
      "      0.42 | [0.56792148]\n",
      "      0.72 | [0.66659861]\n",
      "      0.68 | [0.62110043]\n",
      "      0.52 | [0.48681997]\n",
      "      0.71 | [0.72866535]\n",
      "      0.48 | [0.55339135]\n",
      "      0.47 | [0.38859111]\n",
      "      0.53 | [0.61393012]\n",
      "       0.7 | [0.52293214]\n",
      "      0.78 | [0.76900251]\n",
      "      0.61 | [0.66841559]\n",
      "      0.74 | [0.70857905]\n",
      "      0.71 | [0.7612906]\n",
      "      0.93 | [0.8904418]\n",
      "      0.61 | [0.52089881]\n",
      "      0.53 | [0.69358113]\n",
      "      0.71 | [0.76272539]\n",
      "       0.8 | [0.79742035]\n",
      "       0.9 | [0.88717387]\n",
      "      0.96 | [0.91560099]\n",
      "      0.74 | [0.73018141]\n",
      "      0.94 | [0.90746945]\n",
      "      0.69 | [0.78904658]\n",
      "      0.82 | [0.81306463]\n",
      "      0.46 | [0.35786144]\n",
      "      0.91 | [0.89503402]\n",
      "      0.65 | [0.71260331]\n",
      "      0.71 | [0.81862654]\n",
      "      0.42 | [0.62754825]\n",
      "      0.79 | [0.84986436]\n",
      "      0.91 | [0.87391435]\n",
      "      0.87 | [0.86033269]\n",
      "      0.73 | [0.6090983]\n",
      "      0.78 | [0.80357706]\n",
      "      0.81 | [0.761446]\n",
      "      0.57 | [0.69214931]\n",
      "      0.69 | [0.63161592]\n",
      "      0.87 | [0.8257175]\n",
      "      0.56 | [0.7002088]\n",
      "      0.88 | [0.86488651]\n",
      "      0.81 | [0.85314707]\n",
      "      0.73 | [0.61569617]\n",
      "      0.56 | [0.54897523]\n",
      "      0.68 | [0.67466447]\n",
      "      0.81 | [0.81002233]\n",
      "      0.85 | [0.80437589]\n",
      "      0.76 | [0.72034447]\n",
      "      0.62 | [0.64089811]\n",
      "      0.57 | [0.54633345]\n",
      "      0.83 | [0.85050447]\n",
      "      0.93 | [0.89627148]\n",
      "      0.68 | [0.70715259]\n",
      "      0.78 | [0.80160134]\n",
      "      0.88 | [0.85804985]\n",
      "      0.93 | [0.88469979]\n",
      "      0.64 | [0.65651838]\n",
      "      0.73 | [0.6489664]\n",
      "      0.57 | [0.56756494]\n",
      "      0.45 | [0.47166736]\n",
      "      0.59 | [0.49105568]\n",
      "      0.76 | [0.78637802]\n",
      "       0.9 | [0.87382605]\n",
      "      0.49 | [0.40306032]\n",
      "      0.47 | [0.47021987]\n",
      "      0.77 | [0.73168293]\n",
      "      0.95 | [0.90358701]\n",
      "      0.73 | [0.77451954]\n",
      "      0.55 | [0.81267897]\n",
      "      0.76 | [0.67225346]\n",
      "      0.78 | [0.85625556]\n",
      "      0.77 | [0.79337356]\n",
      "      0.68 | [0.83204863]\n",
      "      0.49 | [0.45329374]\n",
      "       0.9 | [0.85615696]\n",
      "      0.62 | [0.65046797]\n",
      "      0.75 | [0.72054135]\n",
      "       0.7 | [0.68341806]\n",
      "      0.72 | [0.67156431]\n",
      "       0.7 | [0.67989098]\n",
      "      0.67 | [0.62687314]\n",
      "      0.71 | [0.64936495]\n",
      "      0.94 | [0.91527536]\n",
      "      0.72 | [0.71818759]\n",
      "      0.86 | [0.88991116]\n",
      "      0.73 | [0.6305905]\n",
      "      0.34 | [0.39854131]\n",
      "      0.78 | [0.86027264]\n",
      "      0.66 | [0.65576473]\n",
      "      0.54 | [0.43769343]\n",
      "      0.86 | [0.85276278]\n",
      "      0.65 | [0.56569508]\n",
      "      0.71 | [0.73463119]\n",
      "      0.61 | [0.81728732]\n",
      "      0.92 | [0.91338245]\n",
      "      0.67 | [0.73421036]\n"
     ]
    }
   ],
   "source": [
    "def logistic(independentVariable=[1,2,3,4,5,6,7],targetIndex=8,learningRate=0.01):\n",
    "    data=pd.read_csv(\"AdmissionDataset/data.csv\").values\n",
    "    train,test=splitTrainTest(data,70)\n",
    "    \n",
    "    trainX=train[:,independentVariable]\n",
    "    trainX=((trainX-trainX.mean(axis=0))/trainX.std(axis=0))    \n",
    "    testX=test[:,independentVariable]\n",
    "    testX=((testX-testX.mean(axis=0))/testX.std(axis=0))    \n",
    "    trainY=train[:,targetIndex]\n",
    "    testY=test[:,targetIndex]\n",
    "    m=len(trainX)\n",
    "    trainY=trainY.reshape(-1,1)\n",
    "    theta=np.zeros((7,1))\n",
    "    intercept=np.zeros((1,1))\n",
    "    \n",
    "    for epoch in range(50000):\n",
    "        #######\n",
    "        ypred=np.matmul(trainX,theta)+intercept\n",
    "        hx=sigmoid(ypred)\n",
    "        dz=hx-trainY\n",
    "#         loss=logisticLoss(trainY,hx)\n",
    "        dTheta=np.matmul(trainX.T,dz)/m\n",
    "        dintercept=np.sum(dz)\n",
    "\n",
    "        theta=theta-learningRate*dTheta\n",
    "        intercept=intercept-learningRate*dintercept\n",
    "#         if epoch%100==0:\n",
    "#             print(loss)\n",
    "    print(\"=========================================\")\n",
    "    print(\"Intercept: \")\n",
    "    print(intercept)\n",
    "    print(\"Coefficents: \")\n",
    "    print(theta)\n",
    "    print(\"=========================================\")\n",
    "    testPred=np.matmul(testX,theta)+intercept\n",
    "    hxTest=sigmoid(testPred)\n",
    "    print(\"%10s | %10s\"%(\"Actual\",\"Predicted\"))\n",
    "    for a,b in zip(testY,hxTest):\n",
    "        print(\"%10s | %10s\"%(a,b))\n",
    "    \n",
    "logistic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
